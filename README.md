# Primeros pasos con analitica en AWS

![Portada](imagen/cover.png)

 üáªüá™üá®üá± [Dev.to](https://dev.to/elizabethfuentes12) [Linkedin](https://www.linkedin.com/in/lizfue/) [GitHub](https://github.com/elizabethfuentes12/) [Twitter](https://twitter.com/elizabethfue12)

---

## Introducci√≥n üë©üèª‚Äçüíªüëã

**Data Analytics** es un concepto muy utilizado hoy en d√≠a, es casi un pecado no saber que **Data Analytics** es el proceso de recompilar, procesar y analizar datos para usarlos tomar decisiones en base a ellos.

**Data Analytics** te permite combinar datos para crear soluciones que ayuden a las empresas a decidir donde y cuando lanzar nuevos productos, cuando ofrecer descuentos, analizar gastos y buscar ahorros, es posible crear modelos de machine learning para realizar mejoras como customer personalization, detenci√≥n de fraude, alertas en tiempo real, comportamiento de tus usuarios y crear modelos que mejoren financias y predigan la forma de hacer mejores inversiones. 

El **Data Analytics** es importante para las empresas sin importar su tama√±o, sin ello las decisiones serian tomadas por intuici√≥n o suerte. 

La data se puede generar de varias formas, recopilando clics de una p√°gina web, trav√©s de una API, encuestas, bases de datos locales, si sab√©s donde mirar pr√°cticamente todo es data. El desaf√≠o est√° en almacenarla en un mismo lugar, crear Data Lakes, para poder hacer cruces y an√°lisis con ella, hacer uso de ella. 

En este blog te voy a mostrar c√≥mo crear un peque√±o pipeline de **Data Analytics**, donde se deja un archivo en un storage y de ah√≠ es procesado para crear una tabla de la cual se aliment√° un dahsboard de Business Inteligence. 

Y.. Lo podr√°s desplegar listo para usar con un par de comandos usando [CDK](https://aws.amazon.com/es/cdk/?nc1=h_ls) üöÄ üë©üèª‚ÄçüöÄ.

---

## La Soluci√≥n ü§î ‚öôÔ∏è

![fase1](imagen/fase1.jpg)

1. Se deja el archivo en el Bucket de [Amazon S3](https://aws.amazon.com/es/s3/), lo cual activa la [AWS Lambda](https://aws.amazon.com/es/lambda/) que inicia el [AWS Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html).
2. El [AWS Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) explora el nuevo archivo para identificar el esquema (columas), tipo de datos que lo conforman (int, string..etc).
3. Una vez finalizada la exploraci√≥n de [AWS Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) este crea/actualiza la tabla asociada a la data descubierta en el [AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html), el cual a su vez permite que se acceda a la tabla utilizando [Amazon Athena](https://aws.amazon.com/es/athena/). 
4. Queda el DashBoard de [Amazon QuickSight](https://aws.amazon.com/es/quicksight/) listo para hacer Data Analytics.  

---

## Despliegue üöÄ üë©üèª‚ÄçüöÄ


**Para crear la aplicaci√≥n en tu cuenta AWS debes seguir los siguientes pasos:**

### 1. Instalar CDK

Para realizar el despliegue de los recursos, debes instalar y configurar la cli (command line interface) de CDK, en este caso estamos utilizando CDK con Python.

[Instalaci√≥n y configuraci√≥n de CDK](https://docs.aws.amazon.com/cdk/latest/guide/getting_started.html)

[Documentaci√≥n CDK para Python](https://docs.aws.amazon.com/cdk/api/latest/python/index.html)


### 2. Clonamos el repo y vamos la carpeta de nuestro proyecto. 

```bash
git clone https://github.com/elizabethfuentes12/first-steps-with-analytics-in-aws
cd first-steps-with-analytics-in-aws/first-steps-analytics
```

### 3. Creamos e iniciamos el ambiente virtual

```bash
python3 -m venv .venv
source .venv/bin/activate
```

Este ambiente virtual (venv) nos permite aislar las versiones del python que vamos a utilizar como tambi√©n de librer√≠as asociadas. Con esto podemos tener varios proyectos con distintas configuraciones.


### 4. Instalamos los requerimientos para el ambiente de python 

Para que el ambiente pueda desplegarse, debemos agregar todas las librer√≠as CDK necesarias en el archivo  [requirements.txt](https://github.com/elizabethfuentes12/first-steps-with-analytics-in-aws/first-steps-analytics/requirements.txt)


```zsh
pip install -r requirements.txt
```

### 5. Desplegando la aplicaci√≥n

Si deseas desplegar tu soluci√≥n en una region especifica debes modificar el archivo [app.py](https://github.com/elizabethfuentes12/first-steps-with-analytics-in-aws/first-steps-analytics/app.py) la siguiente linea: 

```zsh
env=cdk.Environment(region='us-east-1')
```

Antes de esplegar debemos asegurarnos que el c√≥digo este sin errores, eso lo hacemos con el siguiente comando donde se genera un template de cloudformation con nuestra definici√≥n de recursos en python.

```bash
cdk synth
```

Si hay alg√∫n error en tu c√≥digo este comando te indicara cual es con su ubicaci√≥n.  

En el caso de estar cargando una nueva versi√≥n de la aplicaci√≥n puedes revisar que es lo nuevo con el siguiente comando: 

```
cdk diff
```

Procedemos a desplegar la aplicaci√≥n: 

```
cdk deploy
```

### 6. Tips Para el despliegue


El despliegue lo utiliza utilizando las credenciales por defecto de AWS, si desea usar un profile espec√≠fico agregue --profile <nombre> al comando deploy:

```
cdk deploy --profile mi-profile-custom
```

o simplemente exporte en una variable de entorno

```
export AWS_PROFILE=mi-profile-custom
cdk deploy
```

---

### 7. La aplicaci√≥n

Este CDK crear√° los siguientes elementos:
- Bucket de S3 con el nombre **starting-etl-from-file-inputfilesXXXXXXXXXX**
- Lambda Function con el nombre **process_new_file-STARTING-ETL-FROM-FILE**, el Glue Crawler se inicia con las siguientes lineas de comando dentro de la Lambda.

``` python
    # Comenzamos la ejecuci√≥n del crawler
    response = boto3.client('glue').start_crawler(
        Name= os.environ.get('CRAWLER_NAME')
    )
```

Entra ac√° [Boto3 para glue](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.start_crawler) si quieres aprender m√°s sobre estas l√≠neas. 

- Una base de datos en Glue con el nombre **demo_db**
- Glue Crawler con el nombre **raw-crawler-STARTING-ETL-FROM-FILE**, al correr el crawler se creara una tabla con prefijo **demoetl_**.

Una vez verificado que los elementos se crearon de forma correcta, procede a cargar el archivo en el Bucket de S3 creado, este archivo debe tener formato .csv, .txt o .json, y todos los archivos que cargues despu√©s deben tener el mismo formato y esquema.

Cuando el Crawler vuelva a estado **Ready** significa que la tabla ya fue creada en la base de datos **demo_db**. 

Puedes ver la cantidad de Tablas creadas/actualizadas y adem√°s el tiempo de duraci√≥n del Crawler. 

![crawler](imagen/crawler.png)

En el Cat√°logo de AWS Glue puedes acceder a la base de dato y a las tablas creadas:

![catalogo](imagen/catalogo.png)



---

### 8. Eliminar el stack de la aplicaci√≥n

Esta aplicaci√≥n no elimina el bucket si contiene videos, por lo que primero debes vaciar el bucket y luego proceder a destruir el stak. 


Para eliminar el stack lo puedes hacer via comando:

```
cdk destroy
```

√≥ via consola cloudformation, seleccione el stack (mismo nombre del proyecto cdk) y lo borra.

## ¬°¬°Happy developing üòÅ!!


---


## Adicional ü§î ‚öôÔ∏è üß∞

![fase2](imagen/fase2.jpg)

En este adicional, en vez de dejar un archivo en [Amazon S3](https://aws.amazon.com/es/s3/) se deja en una carpeta de OneDrive. 

1. Se deja el archivo en la carpeta de OneDrive, la cual esta siendo escuchada con un Webhook en [Amazon API Gateway](https://aws.amazon.com/es/api-gateway/). 
2. [Amazon API Gateway](https://aws.amazon.com/es/api-gateway/) activa una [AWS Lambda](https://aws.amazon.com/es/lambda/) encargada de extraer la data de OneDrive y copiarla en S3, para que esto sea posible la Lambda debe obtener el Token y refresh_token desde [AWS Secrets Manager](https://aws.amazon.com/es/secrets-manager/). 
4. Retomamos el paso 1 de la arquitectura de la solucion anterior. 


El c√≥digo para lograr esta solucion se encontrara proximamente en los siguientes enlaces: 

Configuraci√≥n Lambda Funtcion paso 2. 
Configuraci√≥n API Gateway.
Condiguraci√≥n Secrets Manager. 

___

## Servicios involucrados en la soluci√≥n son

### Amazon S3 (Simple Storage Service):
[Amazon S3](https://aws.amazon.com/es/s3/) es un servicio de computo sin servidor que le permite ejecutar c√≥digo sin aprovisionar ni administrar servidores.

### AWS Lamdba: 
[AWS Lambda](https://aws.amazon.com/es/lambda/) es un servicio de computo sin servidor que le permite ejecutar c√≥digo sin aprovisionar ni administrar servidores. 

### AWS Glue Crawler: 
[AWS Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html) Es un servicio de AWS, que te permite desubrir data, reconoce su esquema o columnas, el tipos de dato, arma un catalogo de glue. Se puede ejecutar a demanda o agendados. 

### AWS Glue Data Catalog: 
[AWS Glue Data Catalog](https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html) contiene referencias a los datos, es un √≠ndice de la ubicaci√≥n,al esquema y al tiempo de creaci√≥n. La informaci√≥n en los catalogos se almacena como tablas de metadatos, donde cada tabla hace referencia a un √∫nico almacen de datos. 

### Amazon Athena: 
[Amazon Athena](https://aws.amazon.com/es/athena/) es un servicio de consultas interactivo que facilita el an√°lisis de datos en Amazon S3 con SQL est√°ndar. Athena no tiene servidor, de manera que no es necesario administrar infraestructura y solo paga por las consultas que ejecuta.

### Amazon QuickSight: 
[Amazon QuickSight](https://aws.amazon.com/es/quicksight/) es un servicio de an√°lisis empresarial muy r√°pido, f√°cil de utilizar y administrado en la nube que facilita a todos los empleados de una organizaci√≥n la compilaci√≥n de visualizaciones, la realizaci√≥n de an√°lisis ad-hoc y la obtenci√≥n r√°pida de informaci√≥n empresarial a partir de sus datos en cualquier momento y en cualquier dispositivo.

### Amazon API Gateway: 
[Amazon API Gateway](https://aws.amazon.com/es/api-gateway/) es un servicio completamente administrado que facilita a los desarrolladores la creaci√≥n, la publicaci√≥n, el mantenimiento, el monitoreo y la protecci√≥n de API a cualquier escala. Las API act√∫an como la "puerta de entrada" para que las aplicaciones accedan a los datos, la l√≥gica empresarial o la funcionalidad de sus servicios de backend. 

### AWS Secrets Manager: 
[AWS Secrets Manager](https://aws.amazon.com/es/secrets-manager/) le ayuda a proteger los datos confidenciales necesarios para acceder a sus aplicaciones, servicios y recursos de TI. El servicio le permite alternar, administrar y recuperar f√°cilmente credenciales de bases de datos, claves de API y otros datos confidenciales durante su ciclo de vida. Los usuarios y las aplicaciones recuperan datos confidenciales con una llamada a las API de Secrets Manager, lo que elimina la necesidad de codificar informaci√≥n confidencial en texto sin formato


### CDK (Cloud Development Kit): 
El kit de desarrollo de la nube de AWS (AWS CDK) es un framework de c√≥digo abierto que sirve para definir los recursos destinados a aplicaciones en la nube mediante lenguajes de programaci√≥n conocidos.

Una vez lo conozcas... no vas a querer desarrollar aplicaciones en AWS de otra forma ;)

Conoce m√°s ac√°: [CDK](https://aws.amazon.com/es/cdk/?nc1=h_ls)


---

## ¬°Gracias!

Te dejo mis redes para que me sigas: 

[Dev.to](https://dev.to/elizabethfuentes12)

[Linkedin](https://www.linkedin.com/in/lizfue/)

[GitHub](https://github.com/elizabethfuentes12/)

[Twitter](https://twitter.com/elizabethfue12)

üáªüá™üá®üá±